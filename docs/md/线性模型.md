# 基本形式
* 线性模型一般形式：$f(x)=\omega_1 x_1+\omega_2 x_2+...+\omega_d x_d+b$
    * $x=(x_1;x_2;...;x_d)是由d个属性描述的示例，其中$x_i$是x在第i个属性上的取值

* 向量形式：$f(x)=\omega^T x+b$，其中$\omega =(\omega_1;\omega_2;...;\omega_d)$

* 一个简单的例子：$f_{好瓜}(x)=0.2\cdot x_{色泽}+0.5\cdot x_{根蒂}+0.3\cdot x_{敲声}+1$

* 线性回归的目的是学得一个线性模型以尽可能准确地预测真实结果：$f(x_i)=\omega^T x_i+b$，使得$f(x_i)\simeq y_i$

# 回归问题
* 线性回归：找到一条直线（或一个超平面）使得误差最小

    ![Alt text](image-412.png)

* 均方误差：$E_{(\omega ,b)}=\frac 1m \Sigma_{i=1}^m (f(x_i)-y_i)^2$
    * 解决线性回归问题就是求得均方误差的最小值对应的$(\omega^*,b^*)$

* 平方损失：$L(f(x_i),y_i)=\frac 12(f(x_i)-y_i)^2$

    ![Alt text](image-413.png)

## 线性回归问题求解
1. 方法1：统计学方法（单变量）
    * 数据：$D=\{(x_i,y_i)\}^m_{i=1},其中x_i,y_i\in R$

    * 模型：$f(x_i)=\omega x_i+b,使得f(x_i)\simeq y_i$

    * 策略：平方损失$\mathcal{L}(y_i,f(x_i))=(y_i-f(x_i))^2$，ERM（Empirical Risk Minimization）
        $$
        (\omega^*,b^*)=\mathop{argmin}\limits_{(\omega,b)}\sum_{i=1}^m(y_i-f(x_i))^2
        \\=\mathop{argmin}\limits_{(\omega,b)}\sum_{i=1}^m(y_i-\omega x_i-b)^2
        $$

        * 最小二乘法：基于均方误差最小来对模型进行求解的方法：$目标函数=\sum（观测值-理论值）^2$
    
    * 算法：求解$\omega$和$b$使$E_{(\omega,b)}=\sum_{i=1}^m(y_i-\omega x_i-b)^2$最小化的过程，线性回归模型的最小二乘“参数估计$E_{(\omega,b)}$是关于$\omega$和$b$的凸函数，当关于两参数的导数均为零时得到参数的最优解
        $$
        \frac{\partial E_{(\omega,b)}}{\partial \omega}=2(\omega \sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i)
        \\\frac{\partial E_{(\omega,b)}}{\partial \omega}=2(\omega \sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i)
        $$

        ![Alt text](image-414.png)

2. 方法2：统计学方法（多变量）

    ![Alt text](image-415.png)

    ![Alt text](image-416.png)

    * 必须记忆的公式：
        $$
        \frac{\partial \beta^T x}{\partial x}=\beta\\
        ~\\
        \frac{\partial x^T x}{\partial x}=2x\\
        ~\\
        \frac{\partial x^T Ax}{\partial x}=(A+A^T)x
        $$

    ![Alt text](image-417.png)

    [点击查看公式推导过程](https://zhuanlan.zhihu.com/p/74157986)

    * 解决过拟合问题
        1. 正则化：
            > 模型越简单越好

            > 保持所有参数 $\omega_0,\omega_1,...,\omega_d$，但减少相应参数的贡献

            > 当特征较多时表现较好，每一参数贡献一点到预测y

            ![Alt text](image-425.png)
            
            ![Alt text](image-426.png)

3. 方法3：梯度下降法
    * 梯度下降法的思路：界定目标函数可微，梯度下降算法从空间任一给定初始点开始进行指定轮次的搜索。在每一轮搜索中都计算目标函数在当前点的梯度，并沿着**与梯度相反的方向**按照一定步长移动到下一可行点

    ![Alt text](image-427.png)

    * 梯度下降算法过程：
        * 输入：目标函数$E(\omega)$，梯度函数$\nabla E(\omega)$

        * 输出：$E(\omega)$的极小点$\omega^*$

        ![Alt text](image-428.png)

    1. 批量梯度下降法：
        * 所有的样本都有贡献

        * 可以达到一个全局最优

        * 样本多的情况下收敛速度慢

        ![Alt text](image-429.png)

    2. 随机梯度下降法：
        * 每次更新时用1个样本

        * 计算得到的并不是准确的一个梯度

        * 整体的方向是全局最优解的方向，最终结果往往在全局最优解附近

        * 方法更快，更快收敛

        ![Alt text](image-430.png)

    3. mini-batch梯度下降法：
        * 批量梯度下降法与随机梯度下降法的结合

        * 将所有数据分割成k个mini-batches

        * for each mini-batch k，做一次批量梯度下降法

            ![Alt text](image-431.png)

        * 为每个mini-batch更新参数

            ![Alt text](image-432.png)

# 分类问题
