## 1. 机器学习组成

* 主要任务
    * 分类:将实例数据划分到合适的类别中
    * 回归:主要用于预测数组型数据

* 监督学习
    * 必须确定目标变量的值,以便机器学习算法可以发现特征和目标变量之间的关系
    * 样本集:训练数据+测试数据
        * 训练样本=特征+目标变量(离散值为分类算法,连续值为回归算法)
        * 特征通常是样本集的列,通过独立测量得到
        * 目标变量:目标变量是机器学习预测算法的测试结果(分类算法中通常是标称型,如"真或假";回归算法中通常是连续型,如"1~100")

* 非监督学习
    * 在未加标签的数据中,试图找到隐藏的结构
    * 数据没有类别信息,也不会给定目标值
    * 非监督学习包括的种类:
        * 聚类:将数据集分成由类似的对象组成多个类的过程称为聚类
        * 密度估计:通过样本分布的紧密程度,来估计分组的相似性
        * 无监督学习还可以减少数据特征的维度,以便我们可以使用二维或三维图形更加直观地展示数据信息

* 强化学习
    * 强化学习可以训练程序做出某一决定
    * 程序在某一情况下尝试所有的可能行动,记录不同行动的后果并试着找出最好一次的尝试来做决定

![机器学习基础1](https://note.youdao.com/yws/api/personal/file/WEB55cd2d7f350bd1a843a008817b8716ee?method=download&shareKey=5c64e065a323289d79322c4b9457d957)

## 2. 机器学习专业术语

* 模型:计算机层面的认知
* 学习算法:从数据中产生模型的算法
* 数据集:一组记录的合集
* 示例:对于某个对象的描述
* 样本:也叫示例
* 属性:对象的某方面表现或特征
* 特征:也叫属性
* 属性值:属性上的取值
* 属性空间:属性张成的空间
* 样本空间/输入空间:也叫属性空间
* 特征向量:在属性空间里每个点对应一个坐标向量,把一个示例叫做特征向量
* 维数:描述样本参数的个数(也就是空间是几维的)
* 学习/训练:从数据学得模型
* 训练数据:训练过程中用到的数据
* 训练样本:训练用到的每个样本
* 训练集:训练样本组成的集合
* 假设:学习模型对应了关于数据的某个潜在规则
* 真相:真正存在的潜在规律
* 学习器:模型的另一种叫法,把学习算法在给定数据和参数空间的实例化
* 预测:判断一个东西的属性
* 标记:关于示例的结果信息,如图片里是一只"猫"
* 样例:拥有标记的示例
* 标记空间/输出空间:所有标记的集合
* 分类:预测值是离散值,如把"人"分为"好人"和"坏人"之类的学习任务
* 回归:预测值是连续值,如好人程度达到了0.9之类的学习任务
* 二分类:只涉及两个类别的分类任务
* 正类:二分类里的一个
* 反类:二分类里的另外一个
* 多分类:涉及多个类别的分类任务
* 测试:学习到模型之后对样本进行预测的过程
* 测试样本:被预测的样本
* 聚类:把训练集的对象分为若干类
* 簇:每一个组叫簇
* 监督学习:典范--分类和回归
* 无监督学习:典范--聚类
* 未见示例:"新样本",没训练过的样本
* 泛化能力:学得的模型适用于新样本的能力
* 分布:样本空间的全体样本服从的一种规律
* 独立同分布:获得的每个样本都是独立地从这个分布上采样获得的

## 3. 补充

* 数据集的划分
    * 训练集:学习样本数据集,通过匹配一些参数来建立一个模型,主要用来训练模型
    * 验证集:对学习出来的模型,调整模型的参数
    * 测试集:测试训练好的模型分辨能力

* 模型拟合程度
    * 欠拟合:模型没有很好地捕捉到特征,不能很好地拟合数据,对训练样本的一般性质尚未学好
    * 过拟合:模型把训练样本学习"太好",可能把一些训练样本自身的特性当作了所有潜在样本都有的一般性质,导致泛化能力下降

* 常见的模型指标:
    * 正确率:提取出的正确信息条数/提取出的信息条数
    * 召回率:提取出的正确信息条数/样本中的信息条数
    * F值:正确率`*`召回率`*`2/(正确率+召回率)

* 模型
    * 分类问题:将一些未知类别的数据分到现在已知的类别中去
        * 指标:正确率,召回率,F值
    * 回归问题:对数值型连续随机变量进行预测和建模的监督学习算法
        * 往往通过计算误差来确定模型的精确性
    * 聚类问题:基于数据内部的结构寻找样本的自然族群(即集群)
        * 指标:簇内距离,簇间距离
            * 簇内距离越小越好,也就是簇内的元素越相似越好
            * 簇间距离越大越好,也就是簇间(不同簇)的元素越不相似越好
